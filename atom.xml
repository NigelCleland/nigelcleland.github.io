<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[My Escape]]></title>
  <link href="http://NigelCleland.github.io/atom.xml" rel="self"/>
  <link href="http://NigelCleland.github.io/"/>
  <updated>2013-10-03T20:53:55+13:00</updated>
  <id>http://NigelCleland.github.io/</id>
  <author>
    <name><![CDATA[Nigel Cleland]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Perfection is the enemy of itself]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/09/30/perfection-is-the-enemy-of-itself/"/>
    <updated>2013-09-30T21:51:00+13:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/09/30/perfection-is-the-enemy-of-itself</id>
    <content type="html"><![CDATA[<p>A bold title for a personal essay but a thought which has been kicking around the back of my mind for a while now.
Perfection, as a goal often leads to poorly executed, unfinished, bloated projects.
The very nature of seeking the perfect solution often dooms the searcher to no solution, a less than worthwhile result.
This post will expand my thoughts in this area, as well as illustrating some opposing methodologies which should hopefully serve the reader well.</p>

<!-- more -->


<p>To expand upon my premise I must take a short divergence into some personal anecdotes.
The first anecdote is that I go through transitional periods of both extreme productivity and extreme procrastination.
If I can find a project, or a problem which fascinates me then I can literally lose myself in the moment, and find the oft mentioned sense of flow commonly experienced by deeply skilled practitioners.
This post is now about those moments, those are moments to be husbanded, guarded jealously and ruthlessly exploited when they arrive.</p>

<p>Instead, this essay is about those other moments, those moments which you find yourself on a four hour youtube binge, or reading poorly worded terribly thought out comments on reddit.
This is a post about those periods in my life when I cannot get started, when I lose myself in a mental haze of self destructive melancholy.
I don&rsquo;t like these phases, I consider my time valuable and numerous other individuals have confirmed this to me both monetarily and through the expenditure of their own limited time.</p>

<p>As such, how can I limit my own self destruction?
How can I use my time in such a way as to be in a continuous state of advancement?
The phrase, live each day as if it were your last is disgraceful in its implications.
If a day were truly my last then I would be out buying hookers, blowing through my entire savings and having one of the most amazing parties of my life.
Instead, a better philosophy would be to live each day so that the following day will be better.
Under such a philosophy exercise, education, the pursuit of new skills, forming new relationships are all valuable worthwhile endeavours.
It is from this state of mind that I&rsquo;ll be approaching the rest of this topic.</p>

<p>How does this relate to my thesis topic, that perfection is the enemy of itself?
I implore the reader to think back to their own learning and assess whether the lessons I&rsquo;ve gleaned are of any value to them.
We often hear statements regarding mastery of a particular subject requiring 10,000 hours, in fact we hear this so much it has begun to lose meaning.
I pose the situation of a person who learns the same one hour lesson 10,000 times as compared to the 10,000 different one hour lessons, that is the extreme scenarios.</p>

<p>The person who has studied the one hour lesson a plurality of times will have excellent knowledge of that particular lesson.
However, who is to say that the 10,000th time provided a greater mastery over the particular skill than the 9,999th lesson?
How about as compared to the 5,000th lesson?
The 2,000th?
If we were to plot such a relationship we would discover a clear scenario of diminishing returns, yet this still does not give us any information regarding how many lessons are optimal.
Such an example is absurd but it does serve to illustrate the point that continuous repetition of the same material has negligible value.</p>

<p>We consider the second extreme, of the individual who has learnt 10,000 different lessons.
Here, there exists a number of competing scenarios.
Scenario one, the relationship between mastery and repitition initially follows a steep upward trajectory.
In such a situation each additional lesson provides reduced marginal value as compared to the preceding lesson and therefore the individual gains maximal marginal utility from each possible lesson.</p>

<p>The second possibility is that marginal utility follows a class S-curve pattern.
That is, the initial lesson provides a small amount of utility, and that subsequent lessons will increase marginal utility, to a point, before a taper begins.
This curve is recognisable in many instances of social sciences, for example the product adopters curve (e.g. early adopters to mass market to the last remaining hold outs).
If the lesson utility follows such a curve then the individual who studies many lessons briefly will gain reduced utility.</p>

<p>Both of these scenarios have considered the assumption in that learning one particular skill is independent of any other skills.
However, such a vacuum dependent theory fails to translate appropriately to the real world.
Consider two possible effects, translated knowledge and crowding out.</p>

<p>Translated knowledge occurs when knowledge in one particular domain positively influences knowledge in a second, unrelated domain.
For example, if you read more speculative literature then I will in turn speculative that you have a greater grasp of ambiguities than a corresponding version of you which does not read said literature.
That is, knowledge and learning in one discipline has effected a second domain.
Knowledge in which this general rule applies is incredibly empowering, taken to an extreme this could imply that to get better at things all that is required is a general improvement.
Clearly, at the absurd level this idea falls flat, however I do consider it of some merit and will return to it shortly.</p>

<p>The second scenario, crowding out occurs due to limitations in human short term recall and our ability to process information.
Although the human brains ability to process information is vast, it is not perfect.
Humans can only consider a limited number of scenarios into the future, we often use short term heuristics for decision making and none of us are perfectly rational entities.
Given such prior limitations I consider the case of a human whose brain is continuously saturated with information.
Would we expect such a person to have a high level grasp, of well, anything? Or would we expect such a person to simply be a walking talking collection of randomly assorted facts with no logical capacity to smoothly integrate them into a wider body of knowledge and understanding?
I expect that given too much information the human brain would reject such an overload of information as to be useless.</p>

<p>Thus, given our two scenarios both of which are over 10,000 hours I come to the following general conclusions.
The first is that human knowledge in a particular domain most likely follows a S-curve.
There exists some point, given a purely static field, in which our marginal appreciation will transition through an inflection point, switching from accelerating returns to decelerating returns.
However, this statement is lessened in intensity if a field is continuously changing, although such a field, if the rate of flux is too great, will prevent all mastery from occurring.</p>

<p>The second conclusion reached is that as we increase our knowledge in one particular domain we will expect some of the generalities to provide benefits in other domains.
Although no substitute for specific knowledge in a particular field, this general skill transferral will assist the individual in either accelerating the aforementioned S curve, or providing a solid foundation.
This is broadly the basis of many forms of schooling.
For example as we learn how to form more complete sentences, this partially translates to our ability to write better paragraphs.
However, the ability to write a perfect sentence does not imply the ability to write a perfect paragraph, nor a novel.
The general skill is transferred, not the specific.</p>

<p>The third conclusion reached is that an individual who spreads themself too thin, amongst too many fields, will never truly master any of those fields.
That is, there is a hierarchy of domain mastery which can be expected.
For example, a person could expect to master a single domain, or they could reach a high level of competency in two to four domains.
An individual may be competent in a dozen different domains, but we would not expect the average individual to be a true master in any of those specifically.
Likewise, a person who has spread himself amongst hundreds of fields will only have a cursory knowledge of these fields.
I note that within an individuals knowledge a distribution will exists, that is they will have localised mastery of particular domains.
However, this localised mastery is relative to their mastery of the remaining disciplines.
As compared to a disciplined external party their competence will pale in comparison.</p>

<p>Now, after the not so short sojourn we may return to the thesis at hand, Perfection is the enemy of itself.
Isn&rsquo;t such a statement a form of cognitive dissonance on my part?</p>

<p>Here I qualify this statement through a narrowness of application.
Perfection is the enemy of itself when it is single mindedly applied to a single, specific, task not a general outcome.
For example, consider a software project.
A considerable amount of time may be spent on the project attempting to make a perfect, elegant, even beautiful piece of code.
Something which works, is a pleasure to read and makes you feel like it was a worthwhile sense of accomplish.
In fact, many hours could be spent crafting this particular item, the line by line debugging and optimisation.
The continuous re factoring is ultimately wasted time and resources.</p>

<p>I instead propose a better outcome.
Consider what happens in three months times, when you find that said piece of code no longer fits the use case.
I would guarantee that the same individual in their second attempt would be able to produce an even better piece of work.
Likewise, if three months later on they had to (for whatever reason) do it again.
Now, here I am almost leading down the wrong pathway in that I appear to be suggesting that continuously repeating yourself is a good idea.
It&rsquo;s not.</p>

<p>What is a good idea though is continuously learning from the lessons learnt in the first case and applying those to the subsequent cases.
Broadly, you get better by doing, by continuously expanding your knowledge in a general field through specific applications.
If you are a wood worker seeking to incorporate more curved cuts in your work you do not practice the same cut over and over.
Instead, you seek to cut as many curves as possible, of all shapes and sizes and you learn how your tools work in such situations.
Of course you will need to repeat specific cuts, because there is no substitute for mastery of a single task than repetition of said task.
However, always keep in mind the diminishing returns present, and be aware of the distinctions between adequate, good, great and perfect.
Striving for perfect is not something you plan for, but good and great are worthwhile outcomes.</p>

<p>So far, I have illustrated some of the issues of both too much, and too little, attention upon particular tasks.
Yet, this speaks nothing as to the goldilocks zone, of just right.
Simply, this particular point will vary form task to task, individual to individual but some broad guidelines may be considered.
We may also extend these into a few real world strategies.</p>

<p>We must begin to recognise which knowledge is related within our cognitive sphere of influence.
Look for the similarities between disciplines, don&rsquo;t operate in a vacuum and don&rsquo;t brainlessly repeat tasks repetitively.
It is a special form of insanity to repeat the same task over and over again and expect a different result.
If you are getting different results, you many want to look into functional programming.</p>

<p>Recognise that if you want to get better at a particular skill that it will take a consistent application of time, energy and cognitive resources.
But do not focus these three inputs upon too narrow a field.
I thoroughly enjoy rock climbing, but it is naive to think I will ever progress in the sport if I stick to climbing the same wall, in the same way, over and over again.
However, if I were to practice upon a number of different walls my general skill would improve, if I were to fixate upon a particular wall within the greater subset I would likely be better at that individual one.
Such an analogy is broadly applicable to other situations in life.</p>

<p>Thus, I feel comfortable at this point taking an attempt at addressing my topic.
Perfection is the enemy of itself because a single narrow minded pursuit of a particular, specific, goal limits all second order effects.
By such a narrow field of vision you ignore all possible transferral of knowledge between domains.
Diminishing marginal returns quickly sets in as the mind is stagnant and not exposed to new ideas.
The authors general skill set atrophies as their brain rewires itself for a single task.
Quite simply, seeking perfection on a specific project is like practising the same lesson ten thousand times.
As such, perfection is the enemy of itself, because the individual has self limited themselves.
In effect, by such a single minded focus they have capped the denominator and are seeking to raise the numerator.
By broadening their basis the individual can simultaneously raise both sides of the equation resulting in a better, but still not perfect project.
The single minded pursuit has resulted in a worse outcome than the somewhat lackadaisical general improvement with a slight concentration on the project.</p>

<p>Upon answering the topic I will now seek to expand upon some possible strategies as a result of this.
These strategies are intended primarily for myself.
As I mentioned in the introduction to this essay, I&rsquo;m a terrible procrastinator in certain situations and it is something which I am desperately seeking to fix.
The broad strategies I wish to consider include cognitive displacement, inwards search and active procrastination.
These are names which I have completely made up on the spot and as there is nothing new under the sun I&rsquo;ve likely plagiarised these off someone.
Oh well, I don&rsquo;t care.</p>

<p>Cognitive displacement is a simple principle in that the human body has a limited capacity for in depth learning and knowledge.
That is, the mind may become overwhelmed and will not retain information passed to it.
This is often observed in individuals, who suffering from cognitive overload, stress, anxiety, or a number of other factors experience a degraded mental acuity.
I do not imply that there is a 1:1 replacement between new knowledge and old knowledge, rather that new knowledge is essentially shed before taking root.</p>

<p>Every individual has a different tolerance for new mental experiences.
This tolerance is influenced by factors such as intelligence, education, fatigue, alcohol, stress and noise.
It remains to the individual to thus limit their own cognitive input to ensure maximum retention.
This necessitates the removal of particular sources of information, for example low quality television, radio, print and other mediums.
In the modern world this is doubly difficult.</p>

<p>We live our lives saturated in low value content, and we do not possess the appropriate mechanisms to filtering this content.
In particular, we must devote cognitive resources to assessing the importances of any informations, and what actions are required from this.
Furthermore, if no action is required, and the information is not needed in the past then such activity is an unnecessary expenditure.
On the internet, such forms of low value, non actionable information are prevalent.
Memes, news from foreign countries, domestic scaremongering, celebrity gossip, pseudo scientific articles and other poorly written tripe are all a factor.
Much of this information is designed to draw in the reader, through imagery, sensationalist titling and other parlour tricks.</p>

<p>We thus live in a world where the very act of being aware of our surroundings exhausts our mental faculties.
The danger is that we create individuals with short attention spans, a wide, but shallow knowledge of disparate events and views founded more on sensation than reason.</p>

<p>Moving on from such a dull topic we reach inwards search.
In opposition to cognitive displacement, inwards search is a positive, actionable step which an individual may take to sharpen their focus.
Here, I seek to avoid labels, such as meditation or other labels which people use to dismiss an activity they do not agree with.
Instead, I will call it either inwards search or active focussing.</p>

<p>Enough of definitions and book keeping.
Inwards search is the act of looking within yourself in order to achieve a desired outcome.
There are many ways which people can achieve this and I will speak in generalities here.
To begin, almost by definition, the act requires solitude, and the absence of external simulation in the sensory realm.
I finding a location in which you can control the visual realm e.g. blind folds, dark rooms.
Ideally the place should be quiet and comfortable and be removed from other individuals intrusions.</p>

<p>Once you have found such a location, simply make yourself comfortable and sit or kneel.
The goal is to find a position in which you can remain for at least twenty minutes.
Upon reaching this position, seek to clear your mind of each of the distractions currently weighing upon you.
Empty your mind of the stresses, the worries the mental intrusions.
Instead, focus upon a particular situation, a problem, or simply drift.
Here, be rigid with your thoughts, you don&rsquo;t care about the dishes, laundry or all of the other miscellaneous details you need to cover to live.
During this period of time you must simply be.</p>

<p>Now, how is this different from meditation or other quackery sounding buzzwords.
The simple answer is that it is not.
The long answer is that this time is whatever you want it to be.
All that I am recommending is that you make the time.
Find some peace, quiet and solitude, and simply unwind and be yourself for the day.
You may hate this, you may love this, but either way I highly recommend that you try it at least a few times.
Do note, that initially you will find it very difficult to still your mind to the distractions.
You have most likely ingrained the habit of always being aware deep into your subconscious.
This will take time to rectify.
I personally like to take this time to stretch as well, I know that it&rsquo;s meant to be a period of absolute stillness but I find the deep stretch to be personally helpful.</p>

<p>The next topic of consideration is active procrastinations.
We all procrastinate to differing degrees and we do it in a number of different ways.
Here, we must take a two pronged look at the issue.
The first prong tackles the why of procrastination, it is thoroughly worthwhile to justify to yourself why you are undertaking a particular issue.
You&rsquo;ll likely find yourself unable to answer.
However, the caveat here is that you&rsquo;re most likely going to be in the mood where you just don&rsquo;t care that much.
Thus, it becomes very hard to break outside of the vicious feed back loop which arises.
You procrastinate because you&rsquo;re stressed and worn down.
Because you procrastinated you feel even more stressed, you push yourself harder, become more anxious and get even more worn down.
This causes you to procrastinate more.
Rinse, repeat ad infinitum.</p>

<p>Another name for this is burnout.</p>

<p>How can we tackle this issue?
I want the answer to this question just as much as you do.
I have a very serious battle with procrastination, the feedback loop I was describing above is what I&rsquo;m currently experiencing.</p>

<p>The strategy I&rsquo;m going to try is to take more breaks, earlier.
Broadly, target my productive periods into shortened super intensity periods where I attempt to reach a state of flow.
In the in between stages I want to reach a state of active procrastination, which I will define later.
Intense focus is not required for hour upon hour.
Instead, it is necessary to fully form the mental puzzle of a problem in peace and quiet.
Solve said puzzle, and then let it go from your mind.
Once you have tackled this issue relax and reach into some less cognitively intensive tasks.</p>

<p>What I am currently stuck in is the awful middle ground.
I am never truly at rest.
Yet, I am never truly at my best.
Instead, I meander through a middle ground of low efficiency and poor cognitive awareness.
This stops me from relaxing, exhausts me and wears me down considerably.
This, is my attempt at changing this behavioural pattern.</p>

<p>The second prong is to look at the how we are procrastinating.
What activities are currently being used as a procrastination aid in this respects.
For me, my weakness is rapid content websites such as news sites, reddit, hacker news et cetera.
These sites are like kryptonite to me.
I love being continuously exposed to information, new ideas and new topics.
It stimulates me, and that therein is the problem.</p>

<p>I need to wean myself off my information crack.
To initially accomplish this I&rsquo;m deleting my tablet apps which have this purpose.
Setting up custom hosts files on my computer and installing a website blocker.
I&rsquo;m going to be using the Chrome extension StayFocusd for this.
Ideally the goal is that I just get off the internet in general.
It is good for some things, but it isn&rsquo;t healthy to live your whole life through it.</p>

<p>This is my personal procrastination method.
It has limited value to me personally so I need to replace it.
Here, we introduce the concept of Active Procrastination.
This idea goes hand in hand with the earlier concepts introduced of having a super intensity period followed with lulls.
Here, Active Procrastination serves to fill those lulls.</p>

<p>The following is a list of activities which I consider to be worthy of consideration for the lulls and Active Procrastination.
Your own definitions will obviously differ, so be an adult and write your own list.</p>

<ul>
<li>Weight Training</li>
<li>Cooking something new (Not repeating an easy recipe)</li>
<li>Reading</li>
<li>Writing non seriously</li>
<li>Stretching, Yoga, Meditation, Mindfulness</li>
<li>Music Instruments</li>
<li>Walking</li>
<li>Socialising</li>
<li>Urban Exploring</li>
<li>Hiking/Tramping</li>
<li>Photography</li>
<li>Drawing/Painting/Caligraphy</li>
<li>Programming</li>
<li>Mechanical Maintenance (e.g. Car/Motorbike/Bicycle etc)</li>
</ul>


<p>In addition the following is a list of activities which fit a grey area.
I include them because they definition is not hard and fast and they do improve life in some shape or form.
However, some people do use this as procrastination aids.</p>

<ul>
<li>Cleaning</li>
<li>Rote Cooking (E.g. something simple and easy to make)</li>
<li>Busy work</li>
<li>Shopping (When not taken to excess, everyone needs new clothes)</li>
</ul>


<p>The remainder should be cut from the life where possible.</p>

<ul>
<li>Mindless internet browsing</li>
<li>Rapidly paced content driven websites (e.g. aggregators, news websites, youtube, twitter, facebook)</li>
<li>Excessive masturbation (Key word being excessive)</li>
<li>Pornography usage (Nothing against it personally, it is included as many people, guys especially, use it as a time filler, this may fit a grey area for some people.)</li>
<li>Debating people on the internet. Just don&rsquo;t.</li>
<li>Television</li>
<li>Gossip Magazines</li>
<li>Most forums</li>
</ul>


<p>The reader should begin to notice a clear trend occurring in the three different types.
The goal of active procrastination is replace unhealthy habits with healthier ones.
This time then becomes self improvement time, makes you a more interesting person and should hopefully make you happier.
All of the content on the cut list is essentially modern day soma, cutting you off from the world you live in and replacing it with a fake world of manufactured bliss.
I will write more about these concepts at a later date however.</p>

<p>This I&rsquo;m afraid brings this very long post to a close.
I have rambled off the initial topic somewhat.
That is to be expected when it&rsquo;s late at night and you&rsquo;re reaching close to the 4k word mark however.
I will expand in more depths upon some of the points in this essay at a later date.
I may also revise this essay somewhat as I take the time to proof read it in the future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom cookie cutter python templates]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/09/10/custom-cookie-cutter-python-templates/"/>
    <updated>2013-09-10T17:42:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/09/10/custom-cookie-cutter-python-templates</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve been experimenting lately with custom cookie cutter templates for developing python projects.
Now, what is a cookie cutter template?
Well, it&rsquo;s a simply template which capitalises upon the excellent cookiecutter tool developed by Audrey Roy.
What the tool does is use jinja2 templates in order and a .json file in order to create a basic outline of a project.</p>

<!-- more -->


<p>Sounds simple right?
And indeed it is, especially since Audrey has already developed a master pypackage template which is quite useful for ordinary projects.
However the issue I&rsquo;ve found with most of these is that a lot of the work I do has C dependencies in the scientifc stack, e.g. numpy, scipy, matplotlib, pandas etc.
These dependencies tend to absolutely crap themselves, especially when using Sphinx on an online documentation site such as Read the docs.
This occurs as RTD doesn&rsquo;t build C extensions for you, so you need to make a few special requirements in order to get the imports working properly.
Really frustrating to say the least.</p>

<p>Furthermore, the default Sphinx docstrings method is somewhat archaic and has a numpy of kind of awkward formatting requirements.
What I wanted was a template which would automatically create what I wanted for a data analysis python project.</p>

<p>Ideally I&rsquo;d have the following requirements:</p>

<ol>
<li>Automatic installation of core dependendencies in a virtualenv.

<ul>
<li>Numpy</li>
<li>Pandas</li>
<li>Matplotlib</li>
</ul>
</li>
<li>Updating the Sphinx documentation to be able to handle mathematical equations as well as numpydoc style formatting. This requires the following:

<ul>
<li>numpydoc</li>
<li>mathjax</li>
</ul>
</li>
<li>An updated MakeFile to handle some of the new functionality</li>
</ol>


<p>So, what I&rsquo;ve developed is an updated fork of Audreys pypackage template which handles these updates.
I&rsquo;ve named it, entirely creatively of me, (cookiecutter-pypackage-scistack)[<a href="https://github.com/NigelCleland/cookiecutter-pypackage-scistack">https://github.com/NigelCleland/cookiecutter-pypackage-scistack</a>].</p>

<h1>LaTeX Templates</h1>

<p>Now, given that we can create these excellent Python templates, wouldn&rsquo;t it be great to have the same for LaTeX templates?
A lot of the things required in a LaTeX document are cruft to get set up.
It would be fantastic to not have to deal with this.
Furthermore, one of the biggest issues I&rsquo;ve had is getting multiple formatted versions of the same document up and running.</p>

<p>Now, I also want to use git to manage my documents and version control.
I recommend using bitbucket for this as they provide free private repos.
Thus, it makes sense to create a template which handles the basics.
Now these are still a complete work in progress but I figured I&rsquo;d write about them.</p>

<h2>Enter the cookiecutter-latex-* family.</h2>

<h3>Journal</h3>

<p>The (cookiecutter-latex-journal)[<a href="https://github.com/NigelCleland/cookiecutter-latex-journal">https://github.com/NigelCleland/cookiecutter-latex-journal</a>] package contains a brief template to setting up Journal Articles.
Currently supports a plain and a IEEE style, upcoming styles to support include the Elsevier as well as others which I may find useful.
The goal of this subject is to have a single, primary source of content, e.g. the main.tex file (Note that this may import chapters etc for larger files).
This file is then called by a number of containers which will create the necessary PDFs from this via a MakeFile.</p>

<p>Now the advantages of this template is that formatting and content are separate entities.
Multiple versions of the same document can be produced automatically.
Designed to be used with git for version control for more flexibility and power.
Single unified source of images and tables. (Note that I included a separate tab file as I generate most of my tables from csv files using a custom tool I&rsquo;m going to write about soon once I tidy it up a bit.)</p>

<h3>Still to come:</h3>

<p>Now, I&rsquo;m also looking at extending this family to include more general templates.
My currently anticipated use cases are broadly the Memoir, Report and Beamer classes for books, reports and presentations respectively.
Most critical to me is the Beamer template which I&rsquo;ll most likely implement next.
Would be very cool to have a number of templates automatically implemented for
different styles although the author will likely narrow this down to one style eventually.</p>

<h1>Where to next:</h1>

<p>What I&rsquo;m most excited about with cookiecutter is the upcoming hooks feature.
This feature allows custom scripts to be run both before and after initialising the repo.
Would be great to have this automatically create a repo via the github api, run the initial commit and push the data up automatically to save an additional step.
Rinse and repeat this for read the docs integration and other services as well.
In short lots of interesting stuff.</p>

<p>A lot of my code is still rough as I&rsquo;ve been hit by a lot of stuff lately so I haven&rsquo;t had the opportunity to polish as I would like</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Update]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/08/11/update/"/>
    <updated>2013-08-11T16:06:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/08/11/update</id>
    <content type="html"><![CDATA[<p>So a brief update it most likely due, it has been a substantial period of time since I lost wrote a post.
Although no-one reads this blog so that is all perfectly fine.
Since my last post I&rsquo;ve travelled round Europe for a few weeks, presented at a conference, hit some gym PR&rsquo;s and worked on some exciting elements.</p>

<!-- more -->


<h1>Europe</h1>

<p>What can I say, I had a three week trip around Europe by myself and learnt a lot.
It began in Ireland, starting with the Universitas 21 Energy policy conference in Dublin which was hosted by UCD.
I had a fantastic time, presented on the first day and thoroughly enjoyed the people, and the experience.
The social aspects included a few outings around Irish pubs, a lot of Guiness and plenty of interesting discussions.
The general theme of the conference was Energy with speaks presenting on technical aspects, policy and systems.
There was a very strong distinction between countries with speakers from different nationalities tending to reflect what was important from those countries.
As such, the students from the USA tended to speak about technological solutions, from Europe about social policies, from Asia about poverty alleviation.
All together a fascinating viewpoint as to how different parts of the world view different problems.</p>

<p>From Ireland I travelled to Berlin where I spent five days.
If you&rsquo;ve never been to Berlin, I highly recommend it.
I had an incredible time, the people were fantastic and the sites superb.
I had the best burger of my life in Kreuzberg and enjoyed walking off a couple hangovers at the Berlin Zoo.
I went on a number of tours, met dozens of cool people at the hostels and had a really good time.</p>

<p>From Berlin I travelled to Frankfurt, pretty much on a whim.
One thing I need to mention is that I arrived in Europe with no set plans, no accomodation booked and the only requirement being that I make a flight back to NZ from Paris.
As such, I ended up in Frankfurt with an open mind.
My hostel was located in the red light district which was pretty hilarious as I kept questioning if I was in the right place.
Nonetheless, I felt safe here.
Frankfurt the city has a number of gorgeous museums, cool bars, great people and interesting architecture.
The Frankfurt city forum was on at the time and a lot of sausages were eaten at the street stalls.
One of my favourite places was the Goethe museum which was a little three story building.
The most amazing German realist naturalist paintings from the mid 19th century were inside.
I was completely blown away.</p>

<p>From Frankfurt I travelled to Cologne where, after not really knowing the cathedral existed, I had quite a shock after walking out of the train station.
Cologne was a very cool city, the shopping district was really well put together, the architecture interesting and the people really nice.
Not to mention the beer here was, cheap, delicious and plentiful.</p>

<p>Leaving Frankfurt I travelled to Paris where I had a mixed experiment.
By this point in the trip I was getting a little worn down.
Travelling non-stop for 2 weeks straight had led to a lot of blisters, sore feet and a lack of clean clothes.
The city itself was incredibly spectacular with the view from the top of the arc de triomphe a must see.
However the city itself was expensive, jammed full of tourists and the general vibe from the people was one of rudeness.
So far in my trip I would have happily returned to any of the other cities.
However, I don&rsquo;t feel any particular desire to travel to Paris again.
It was just simply not enjoyable.</p>

<h1>Work</h1>

<p>It was a bit of culture shock getting back and needing to work again.
It did take me a few weeks to get into it but now I&rsquo;m back into the swing of things.
My current projects include an assessment of spot price dynamics, an assessment of wind energy and a look into simulations regarding Pole 3.
All in all, a lot of work on and I&rsquo;m excited about my current research.
A big push has been doing a little a lot.
Although I typically work best in bursts, with periods of extreme activity followed by inactivity I&rsquo;ve been attempting to mitigate this.
One element of this is not trying to &ldquo;force&rdquo; the work but simply to switch tasks rather than becoming stuck.
I&rsquo;m still not accomplishing this as well as I would hope but I&rsquo;m making progress.</p>

<h1>Other</h1>

<p>I&rsquo;ve been continuing my reading with book depository doing very well by me.
I&rsquo;m still in a physical book stage, with ebooks currently not taking my fancy.
Although I own multiple computing devices, tables, kindles etc the recent security revelations, plus decision making of American IT companies has broadly soured me on entrusting my data to them.
I&rsquo;ve been assessing moving away from their stranglehold as I see little reason to continue using their services.
A big one has been migrating more of my document storage and back ups away from Dropbox to a three way synchronisation using BT sync and a raspberry pi.
I still need to complete this however.
I&rsquo;m looking into alternatives to gmail with mailpile being a possible alternative.
However at this stage it is looking as though Google will still be getting my service.</p>

<p>On another note I&rsquo;m very excited about Ubuntu touch and I&rsquo;m looking forward to flashing an image on to my Nexus 7.
The Ubuntu edge campaign has been interesting to follow but it appears as though it will fail.
In addition, given the exchange rates the phone was just priced slightly outside what I&rsquo;m willing to pay.
For my next phone I will most likely be getting a Nexus 4, which I&rsquo;ll attempt to put Ubuntu touch on.</p>

<p>I&rsquo;m currently considering what I want my next personal project to be.
It&rsquo;s a bit of a struggle balancing my time appropriately as between work, gym, life admin and reading it can be quite limited.
I&rsquo;m looking into learning either Haskell, or possibly more about web development including D3.
D3, HTML and CSS look like they&rsquo;ll be more relevant at this stage but the premise of functional programming and Haskell look quite fascinating.
I need to bite the bullet and choose one of these soon though however.</p>

<p>In the gym things have been going fantastic.
I&rsquo;ve been hitting multiple PR&rsquo;s.
Bought some new squat shoes and have been really enjoying it all.
Currently very happy with how things are progressing although I do need to work on my mobility a bit more.</p>

<p>This has gotten long enough now so I&rsquo;ll just leave it here.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Bittorrent Sync]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/06/07/on-bittorrent-sync/"/>
    <updated>2013-06-07T12:21:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/06/07/on-bittorrent-sync</id>
    <content type="html"><![CDATA[<p>So Bittorrent Sync is the newest offering from BitTorrent and I must say I am in love. Broadly, it allows you to synchronise folders between multiple computers quickly and easily. In this way it is very similar to Dropbox and other equivalents (Drive, SkyDrive, Box, iCloud) etc. Now those other services have their own lists of benefits and negatives which I&rsquo;m not going to cover here. Instead, this post is about BitTorrent Sync.</p>

<p>First, Bittorrent has been given a bit of a bad rep in recent years due to its proliferation in the use of internet piracy. However, such a use is technology agnostic and a number of other individuals and companies are using BitTorrent for entirely legitimate purposes. Simply, it reduces the requirement for centralised servers for downloading and sharing files between multiple people. In effect, cutting down on costs and bandwidth requirements for people who have many large files to share.</p>

<!-- more -->


<p>Notable users include many linux distributions, World of Warcraft and the BBC iPlayer (these are all from memory, they may have changed). Simply by redistributing the burden of distributing files to the community multiplier network effects can be induced. For example, the more people who are sharing a particular download the fast that distribution can be expected to occur. As many people do not fully utilise their bandwidth and connections such sharing is a simple way of piggybacking off the existing infrastructure.</p>

<p>Other benefits include the potential to use local versions (instead of grabbing a copy off the US and using limited international bandwidth) the abundant local fibre networks can be used. This is similar in principle to an ISP maintain a local Content Delivery Network for large downloaded files (for example streaming services/steam/etc). So what on earth do these two things have in common?</p>

<p>Enter BitTorrent Sync.</p>

<p>BitTorrent Sync incorporates the power of the bittorrent protocol along with the ease of use of Dropbox and its ilk. Furthermore, as everything is local it has the following advantages.</p>

<ol>
<li>You can sync everything over a LAN if available (e.g. don&rsquo;t need to use limited bandwidth)</li>
<li>No size limits!</li>
<li>Can share with as many, or as few, people as desired making it perfect for collaborating on documents etc where a full version control system such as git would be overkill.</li>
<li>All files are stored locally so you can safely share sensitive information without use of a central server.</li>
<li>Control</li>
<li>One way synchronisation</li>
</ol>


<h2>Syncing over LAN.</h2>

<p>This one here is crucial for me. In New Zealand we have very limited, very expensive bandwidth caps which make the constant syncing of large changes to DropBox folders a major pain in the proverbial. Being able to Sync everything over LAN means I can set up my backups incredibly simply (I use a RPi powered set up for this) and generally just take the hassle out of everything.</p>

<h2>No Size Limits!</h2>

<p>I have GBs of raw data which I&rsquo;m continuously doing analysis on, sometimes on different computers and platforms. Simply having a consistent space for all of my data so that I can be assured that what works on one computer will work on another is an absolute pleasure.</p>

<h2>Unlimited, personalised sharing</h2>

<p>I know I know DropBox etc can do this to. However it&rsquo;s still a major plus that I can set up a customised sharing lists both with myself and with other people (e.g. different folder sharing to share photos with friends and loved ones). I&rsquo;m not personally a fan of the &ldquo;put everything on Facebook&rdquo; style of sharing and prefer to share high quality original files where appropriate. This makes it easy, and as an added bonus I keep control of my files and documents.</p>

<h2>Local Versions only</h2>

<p>All BitTorrent sync traffic is fully encrypted. Thus, you can feel safe that all files shared are only going to be available to those people you want to see them. No worries about leaving things on usb drives, or sending them via email attachments. Simply just share the folder or file directly with bittorrent sync.</p>

<h2>Control</h2>

<p>I know a lot of people don&rsquo;t care about this, however from a personal point of view I adhere to the &ldquo;If the product is free, then you are the product&rdquo; mantra of the internet. Being solicited for advertisements based upon the files I&rsquo;m sharing for work/personal purposes is not something I ever want to happen in the future. Nor do I have to worry about what happens if a Service such as DropBox or Google Drive change their terms of service. As such I&rsquo;m much less locked into the Sync platform than the others, this low friction environment is preferential for personal reasons to me.</p>

<h2>One Way Synchronisation</h2>

<p>There are many occasions when I want someone to be able to see what I&rsquo;m working on, but don&rsquo;t want them to modify my source. If two way synchronisation is enabled by default then I have to maintain my own local version to ensure that someone doesn&rsquo;t accidentally wipe my shared version. This is a major PITA and a source of friction and can often lead to the dreaded filename_author_date_version_latest_seriously_SERIUOSLY.ext filenaming system which I have seen utilised quite often. Quite simply this just isn&rsquo;t an acceptable way of conducting business. At all.</p>

<h2>Convinced yet?</h2>

<p>Convinced? Give it a try, it&rsquo;s free! You can find the download and other material at the <a href="http://labs.bittorrent.com/index.html">BitTorrent Sync</a> website.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Custom LaTeX chapter heading with image]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/06/03/custom-latex-chapter-heading-with-image/"/>
    <updated>2013-06-03T19:32:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/06/03/custom-latex-chapter-heading-with-image</id>
    <content type="html"><![CDATA[<p>A brief command to incorporate an image as part of a chapter heading, and position it so that it makes sense. This took about 30 minutes of googling to pull together the appropriate pieces, not a great use of time but I learnt a couple new LaTeX tricks while I was doing it so that&rsquo;s all good.</p>

<!-- more -->


<p>What I wanted was the ability to include an image directly above a chapter title in a LaTeX report. This was so that I could define both a chapter title and image and have everything just &ldquo;work&rdquo; and be scaled appropriately. I should note, that this technique works best with wide screen images. In particular, for consistent sizing of the images each one should have the same resolution (otherwise it will scale oddly).</p>

<p>Furthermore, the command also supports an appropriate inclusion in the table of contents which is nice and means we can just use it as a stand alone command.</p>

<p>This is best demonstrated via example as follows</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='latex'><span class='line'><span class="k">\documentclass</span><span class="na">[12pt, a4paper]</span><span class="nb">{</span>report<span class="nb">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">\usepackage</span><span class="nb">{</span>titlesec<span class="nb">}</span>
</span><span class='line'><span class="k">\usepackage</span><span class="nb">{</span>geometry<span class="nb">}</span>
</span><span class='line'><span class="k">\usepackage</span><span class="nb">{</span>graphicx<span class="nb">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">\newcommand*</span><span class="nb">{</span><span class="k">\imgchapter</span><span class="nb">}</span>[2]<span class="nb">{</span>
</span><span class='line'>  <span class="k">\refstepcounter</span><span class="nb">{</span>chapter<span class="nb">}</span>
</span><span class='line'>  <span class="k">\newgeometry</span><span class="nb">{</span>top=0cm<span class="nb">}</span>
</span><span class='line'>  <span class="k">\addcontentsline</span><span class="nb">{</span>toc<span class="nb">}{</span>chapter<span class="nb">}{</span><span class="k">\protect\numberline</span><span class="nb">{</span><span class="k">\thechapter</span><span class="nb">}</span>#1<span class="nb">}</span>
</span><span class='line'>  <span class="k">\chaptermark</span><span class="nb">{</span>#1<span class="nb">}</span>
</span><span class='line'>  <span class="k">\begin</span><span class="nb">{</span>center<span class="nb">}</span>
</span><span class='line'>  <span class="k">\makebox</span><span class="na">[\textwidth]</span><span class="nb">{</span><span class="k">\includegraphics</span><span class="na">[width=\paperwidth]</span><span class="nb">{</span>#2<span class="nb">}}</span>
</span><span class='line'>  <span class="k">\end</span><span class="nb">{</span>center<span class="nb">}</span>
</span><span class='line'>  <span class="nb">{</span><span class="k">\normalfont\huge\bfseries</span>
</span><span class='line'>  <span class="k">\chaptertitlename\ \thechapter</span>:  #1<span class="nb">}</span>
</span><span class='line'>  <span class="k">\restoregeometry</span>
</span><span class='line'><span class="nb">}</span>
</span><span class='line'>
</span><span class='line'><span class="k">\begin</span><span class="nb">{</span>document<span class="nb">}</span>
</span><span class='line'><span class="k">\tableofcontents</span>
</span><span class='line'><span class="k">\imgchapter</span><span class="nb">{</span>This is a test chapter<span class="nb">}{</span>test.png<span class="nb">}</span>
</span><span class='line'><span class="k">\end</span><span class="nb">{</span>document<span class="nb">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>And we can also check out the finished product as well</p>

<p><img src="http://NigelCleland.github.io/images/june2013/chapter_test.png"></p>

<p>As you can see we&rsquo;ve introduced an image above our chapter heading, the table of contents is still all set up and generally life is good.
Now, if the above code isn&rsquo;t working properly please make sure the appropriate packages are installed. Additionally, I did have to remove a few percentage signs as they appeared to be making the ruby parser bork or something along those lines.</p>

<p>One thing you may want to change is that I prefer the Chapter X: chapter title to all be on a single line. Whereas the default LaTeX style has them on separate lines.</p>

<p>Sources, aka where the real credit lies</p>

<p>Image Manipulation:
<a href="http://tex.stackexchange.com/a/3548">Will Robertson</a>,
<a href="http://tex.stackexchange.com/a/39148">Werner</a>,
<a href="http://tex.stackexchange.com/a/35865">Schweinebackge</a></p>

<p>Chapter Changes: <a href="http://tex.stackexchange.com/a/25031">Alan Munn</a></p>

<p>Margins: <a href="http://stackoverflow.com/a/16259351">Kevin Chen</a></p>

<p>Hope this is helpful</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Experimenting with Flask and Web Development]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/06/01/experimenting-with-flask-and-web-development/"/>
    <updated>2013-06-01T18:09:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/06/01/experimenting-with-flask-and-web-development</id>
    <content type="html"><![CDATA[<p>I&rsquo;m currently experimenting with using Flask for a bit of Web Development.
I&rsquo;ve never done web dev before and it has been an interesting learning experience so far. I&rsquo;m going to create a fairly simple tracking app for the gym etc, basically get a Database set up and some logging going on.</p>

<!-- more -->


<p>Once this is done, I&rsquo;ll try and extend it and get some fancier features going on, visualisations etc. It will be interesting working with these to see how python can be used in such a fashion. May end up dictating some of the other work I do as well which I may want to expose via websites.</p>

<p>This post is currently a work in progress, where I&rsquo;m up to so far</p>

<h2>Flask</h2>

<p>Flask is a python micro framework which is used for web applications.
It&rsquo;s small, contains very few lines of code, and relies heavily upon two other libraries Werkzeug and Jinja. I&rsquo;m currently implementing the project using a module type approach as follows</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>trackme/
</span><span class='line'>  runserver.py
</span><span class='line'>  trackme/
</span><span class='line'>      static/
</span><span class='line'>          style.css
</span><span class='line'>      templates/
</span><span class='line'>          # templates
</span><span class='line'>      __init__.py
</span><span class='line'>      database.py
</span><span class='line'>      views.py
</span><span class='line'>      models.py
</span><span class='line'>      forms.py</span></code></pre></td></tr></table></div></figure>


<p>Currently:</p>

<ul>
<li>models contains the SQLAlchemy tables</li>
<li>database contains the SQLAlchemy setup infor</li>
<li>views contains the flask page views</li>
<li>forms contains the WTForms Forms</li>
<li><strong>init</strong>.py contains the app itself</li>
</ul>


<h2>SQLAlchemy</h2>

<p>SQLAlchemy is a library which is used to create a bit of magic when it comes to SQL databases. Currently I&rsquo;m just running a local SQLite database and I&rsquo;m not too too worried about anything surrounding this as this is just a personal fun project at the moment.</p>

<h2>WTForms</h2>

<p>WTForms, this is designed to make Forms far far easier to use. I&rsquo;m still getting my head around a lot of this but I&rsquo;ve made good progress so far.</p>

<h2>Templates: Jinja 2</h2>

<p>I&rsquo;m currently not really sure how to create my own templates, nor what these are really doing but it&rsquo;s something I plan on learning more about as I go.</p>

<h2>CSS</h2>

<p>I don&rsquo;t really know a hell of a lot about CSS but I guess I&rsquo;m going to have to find out!</p>

<h2>Plotting</h2>

<p>I&rsquo;m currently not to sure what I&rsquo;m going to use for plotting things out, I&rsquo;m currently leaning towards either d3.js or vincent. I&rsquo;ve been looking for an excuse to try something apart from matplotlib for awhile now so will just have to see how this goes.</p>

<h2>To do:</h2>

<p>Wow, the list of things to actually get done to make this up and running are intense. I guess I can separate these into broad functionality steps.</p>

<ol>
<li>Ability to add workouts quickly and easily
1a. Begin writing tests to ensure that everything works</li>
<li>Ability to view workouts easily</li>
<li>Add support for profiles</li>
<li>Figure out templates, more html and templates to beautify the app a touch.</li>
<li>Implement graphical plots for progress</li>
<li>Add measurements, e.g. weight/size etc to help track progress</li>
<li>Begin extending the app to track other aspects (e.g. food/stretching/w.e.)</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What I'm currently working on]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/27/what-im-currently-working-on/"/>
    <updated>2013-05-27T19:52:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/27/what-im-currently-working-on</id>
    <content type="html"><![CDATA[<p>Haven&rsquo;t been able to write as much as I would have liked to over the past week. Been a bit tied up with other things going on. Thus, I thought I&rsquo;d take this opportunity to run through each of the various projects and research lines I&rsquo;m going down at the moment. Hopefully this should be a bit of an overview as to what I actually do.</p>

<p>Broadly I&rsquo;ll divide my projects into three categories, write up stage, in progress stage and backburner stage. A project in write up stage is currently undergoing final revisions before submission to journals/conferences etc. Ones in the in progress stage are what I&rsquo;m currently actively developing. This includes a lot of experimentation and prototyping to help consolidate my thoughts. Finally, the ones on the backburner stage are those which I&rsquo;m currently thinking about and will tackle when I get time.</p>

<!-- more -->


<h2>Write up Stage</h2>

<h3>Reserve Constraints in co-optimised energy and security markets</h3>

<p>My first major piece of work which started it all. In an energy and reserve co-optimised market the total least cost solution for energy and security are dispatched. However, this can result in some quite non-intuitive dispatches and final pricing which is difficult to understand.
In particular, the merit order appears to be broken and final pricing can become disconnected from the energy offers.</p>

<p>This body of work discusses the theoretical mechanism through which this occurs. These theoretical mechanisms can be developed in two separate ways. Using the associate dual program (to the primal dispatch problem) or by developing small models which have a small subset of the constraints applied. I prefer the second approach as it is quite intuitive when visualising the solution upon a small model as compared to the mathematical result which arises. These constraints can occur in a number of different ways, and corner point solutions are also possible.</p>

<p>Finally, I apply the results to create a set of filters which can be applied to the New Zealand market to determine when constraints occur.
These filters are extremely effective (although transmission losses are an issue) at identifying the vast majority of the constrained periods. These results are illustrated as well as some discussion regarding the effect upon average prices discussed. The broad results is that security co-optimisation does not lead to any noticeable (long term) effect on energy prices although seasonal effects are apparent. However, these constraints have a major effect upon the average reserve market price and their resolution may severely reduce such pricing.</p>

<h3>Probabilistic models for assessing the frequency of reserve constrained events.</h3>

<p>This is a body of work which extends upon the reserve constraints I identified above. Broadly, it seeks to investigate the common causes and if possible develop a probabilistic approach to investigating when they might occur. Broadly, I assess the general factors which are related in Electricity markets, time of day, time of year, hydrology, demand, security availability and market risk. From these factors I choose a subset which I then develop into an extended model.</p>

<p>The model used inverse weighting of two smaller, more generic predictions. The inverse weighting method is used due to a dichotomy in the number of periods which exist at different hydrology levels. Using discrete bin sizes would lead to large errors and horribly distort the model. However, it was non-intuitive as to the best method of rectifying this. I settled on the inverse weighting approach as it appeared simple and elegant and accurately addressed my major concern.</p>

<p>The model develop shows great qualitative and adequate quantitative accuracy. These events in New Zealand are uncommon, happening between 2% and 10% of the total trading periods over the past five years (South Island market, North Island market respectively). Thus, any approach would provides an indication as to when they occur is very useful. Broadly, the model was able to predict when large numbers of constrained periods would occur, as well as when they would not. Such a result is a definitive increase compared to a simple naive average, furthermore it is an enhanced understanding over the raw (univariate) analysis initially conducted.</p>

<h3>On the non-linearity of consequences, why low probability events matter in Electricity networks.</h3>

<p>This is a short extended abstract and presentation which I&rsquo;m preparing for the U21 graduate conference in Dublin. The theme of the conference is Energy Policy and Systems. In this vein I&rsquo;ve decided to not speak about security constrained events and to take a more philosophical approach. One line of thought which I find fascinating is that some events are so catastrophic, so disastrous that people would pay almost any price to avoid them.</p>

<p>I extend this idea into the realm of Electricity markets and develop several case studies. These case studies explore the broad elements of non-linearity in bounded systems (e.g. how the system behaves as it approaches the bound, as well as the location of each bound) and discuss their importance. It is also extend to develop a non-linear tolerance between different participants in a system to highlight a dichotomy. The broad case studies discussed are:</p>

<ul>
<li>Cascade failure in Electrical networks leading to black starts</li>
<li>Extreme price distributions, 53% of revenue in the NZ security (secondary) market is made in 1% of the trading periods</li>
<li>Response to the Christchurch Earthquake of 2011 which highlighted different participants tolerance to network disruption.</li>
</ul>


<h2>In Progress Stage</h2>

<h3>Hedging via Reserve Markets</h3>

<p>This is an idea which I&rsquo;m currently developing regarding the effectiveness of partial hedging via the reserve markets. Broadly, such a measure is similar to a Financial Transmission Right (FTR). However, an FTR is far more comprehensive than the Reserve Hedge. The pertinent question to be answered would be the relative effectiveness of such a measure. This effectiveness will likely be measured by assessing the degree of cover afforded by such a hedge as well as the relative cost of such a measure.</p>

<p>I may be able to incorporate my probabilistic work into this to extend it into a dynamic hedging strategy. This could be undertaken along both perfect (full information and ability to hedge) and imperfect (heavily constrained) viewpoints. Either way, it will be an interesting assessment of how reserve markets can be linked into the energy markets. One measure which is not covered is that reserve hedging could be accomplished either physically or financially.</p>

<h2>Backburner Stage</h2>

<h3>Offer Strategies for participants during reserve constrained events.</h3>

<p>This will be more of a theoretical evaluation as to how optimal offer strategies begin to change when Reserve constraints are present in a market. These constraints will be assessed from a number of view points, e.g. generator, generator with reserve, reserve provider etc as to determine what the effect is on the margin. As there exists a linkage between the energy and reserve market prices during these events. Thus, it may be possible to highlight when a participant may be able to exert market power in one market to induce a constraint resulting in increased profits.</p>

<h3>Simulation study of the effect of reduced Transmission constraints upon Reserve and Enerfy pricing in New Zealand.</h3>

<p>My current body of work has shown that there exists a substantial relationship between the value of the security market and constraints. Broadly, the market is defined by extremely low median prices with very high priced tail events which occur extremely rarely. However, these events are linked to constraints which exist due to the configuration of the HVDC poles. With the commissioning of the new Pole it will be worthwhile to assess what effect this will have upon the marginal periods when electricity peakers and reserve providers expect to recover the majority of their costs.</p>

<h2>Code Projects</h2>

<h3>nzem</h3>

<p>This is intended mainly as a personal repository, but I will attempt to make it usable for others, which should investigate the ease of investigating the NZ electricity market. It currently has some broad helper functions but is currently in a bit of a haphazard space and needs a full rework soon. I&rsquo;ll try and put together a longer post explaining where I&rsquo;m intending to take this as well as highlighting what work needs to be done soon.</p>

<p>I also want to extend this module to help assess the NZ electricity hedge market including a better method of handling the data import and output. I&rsquo;ll attempt to get this started as soon as possible.</p>

<h3>pdtools</h3>

<p>pdtools is a repository which applies a few monkey patches to the Pandas DataFrame and Series objects. Currently it adds some useful (in my opinion) data selection functions to both DataFrames and Series as well as providing method to simplify merging series. This repository will contain the functions and shortcuts I find most useful when working with Pandas objects.</p>

<p>One thing which I do want to extend with this library is making the plotting and data output slightly nicer for including in publications. I find the default matplotlib styles a little ho-hum so I&rsquo;ll try and build some improve functionality into the repository to make the plots prettier. One other aspect is that I prefer a particular LaTeX table style when I&rsquo;m putting together tables so I&rsquo;ll include this as well.</p>

<h3>pyspd</h3>

<p>pyspd is a small repository which is intended to create small linear programs with the full complement of security and risk constraints in the NZ market. Broadly, it provides wrappers for setting up Nodes and generation stations without needing to understand how the underlying mathematics work. To do this it builds upon the puLP repository and the relevant solvers will be needed.</p>

<p>Currently this repository needs a lot of work to bring it up to scratch in the near future.</p>

<h2>Conclusion</h2>

<p>Well, this broadly covers everything I&rsquo;m working on professionally at the moment. There are a number of things which require my attention and it can be difficult juggling the time appropriately between them. In particular, I find when I get in a state of flow that I can crank out a large volume of highly productive and high quality work in a few hours. However, if I can&rsquo;t reach this state then it is difficult to be productive. One thing I&rsquo;m working on is trying to queue up work which can be done in less productive states (i.e. requiring little creativity or insight) so that I can still get things done. However, I appear to have had limited success at doing this so far.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why Researchers should strive to develop repeatable code]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/20/why-researchers-should-strive-to-develop-repeatable-code/"/>
    <updated>2013-05-20T11:06:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/20/why-researchers-should-strive-to-develop-repeatable-code</id>
    <content type="html"><![CDATA[<p>As a researcher you&rsquo;ll often have to conduct a lot of smaller experiments, testing out a hypothesis here, running some data analysis there. Yet a lot of people I know, and I am personally guilty of this as well, do not make these scripts repeatable. Writing smart, testable, repeatable code takes longer and for many researchers who code out of necessity, not out of love it is not as intuitive. In many undergraduate degrees, particularly in engineering, the goal throughout the course of study is more to finish the assignment, pass the paper and repeatability be damned.</p>

<p>However, this behaviour becomes counterproductive once you start researching. I cannot imagine the number of hours I have wasting having to repeat useless boiler plate, hard coded examples when some intelligent preparation work, although taking a bit longer initially would save me far more time in the long run.</p>

<!-- more -->


<p>The temptation is to justify taking the quick route through the rational realisation that a lot of the code you write will often be thrown away. Therefore, what is the point in making sure it is repeatable if we may throw out the hypothesis and start again anyway? Here, focus upon the methods you are applying, know that data you are applying it to and take a couple lessons from functional programming.</p>

<p>For example, you may create a great hypothesis which works for your current set of data. However, you&rsquo;ve screwed up and hard coded a bunch of exceptions, magic numbers and other such parameters into the code. Elements you didn&rsquo;t need to. Wouldn&rsquo;t it be great if you could simply point your magical, world breaking, code at a new source of data and let the computer sort it all out?</p>

<p>In my opinion this is what researchers should be striving to do. To separate content and form so to speak from one another. If you are developing a model, generalise the parts of it and then set it up so they are easy to connect. The model itself should update itself depending upon what data you feed it and shouldn&rsquo;t be limited as such. By arbitrarily limiting it you&rsquo;re really just kicking yourself later on down the road.</p>

<p>So, how can we avoid screwing up, and start developing elegant repeatable code? Let&rsquo;s go through a couple options.</p>

<ul>
<li>Folder structure and file naming</li>
<li>Version Control</li>
<li>Smarter development of objects and functions</li>
<li>Modules and packages</li>
<li>Tests</li>
</ul>


<h2>Folder Structure and File Naming</h2>

<p>Folder structure, or directory layout, seems like a strange thing to be placing upon a list of developing elegant code. Same with file naming. So what does this have to do with research and developing code? Quite simply a lot. Let&rsquo;s talk about the initial folder layout I use a simple three tier approach as follows.</p>

<p>Note, that here the data is for trivial examples. If you&rsquo;re using anything more complex then I highly recommend you start researching into Databases and whether your data is suitable for them. However, one caveat is that introducing a Database layer may add significant overhead and if you are unskilled this may be beyond you. Nonetheless, give it a try, see if you like it. As a general rule of thumb think about how much information you are needing to store. 10 MB, you may be okay with a file. 100 MB, pushing it, possibly okay if you&rsquo;re using a library like pandas and vectorised calculations. 1000 MB, Databases start becoming your friend very very quickly. Additionally, you should ask yourself whether other people will also need access to the data sources.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>data
</span><span class='line'>  - set_one
</span><span class='line'>  - set_two
</span><span class='line'>scripts
</span><span class='line'>  - experiment_one
</span><span class='line'>  - experiment_two
</span><span class='line'>modules
</span><span class='line'>  - module_one
</span><span class='line'>  - module_two</span></code></pre></td></tr></table></div></figure>


<p>Now, what I&rsquo;ve done is I&rsquo;ve completed separated the three key aspects of my work. All of my data is contained within its own folder. I don&rsquo;t touch it and I certainly don&rsquo;t change any of the files within once they&rsquo;ve been modified. What this ensures is that if I chose to repeat something I can be sure that my dataset hasn&rsquo;t changed, and that assuming that the code has changed my program and code should just <em>work</em>.</p>

<p>Secondly, I&rsquo;ve separated out my scripts and modules. Here we should define what each is. A module is something you&rsquo;ve developed which is testable and method independent. It may be a package you&rsquo;ve got off <a href="github.com">github</a> or something you&rsquo;ve developed yourself, such as a useful package of <a href="github.com/NigelCleland/pdtools">tools</a>. Now, your scripts on the other hand are the nitty gritty of what you do. You can still write these in a terrible fashion however at least the tools you use, and the data you run it on should be easily separable.</p>

<h2>Version Control</h2>

<p>What is Version Control and why, as a researcher, do I care? Ever worked with other people and what ends up getting passed around is a ton of files with names such as <code>file_name_person_number_44_date_37_final_copy_3.ext</code>. If yes, and if this made you absolutely hate life and become seriously concerned about the future well being of the human race then version control is for you!
Now, I recommend using what people are comfortable with, however if you&rsquo;re new to it then my recommendation is to pick up git.</p>

<p>Git is a decentralised version control system which makes merging together branches etc easy. Don&rsquo;t know what any of that means? Then pick up a copy of the progit book from github and get reading. I&rsquo;m not going to go through all of the nitty gritty here, just to say that you should be using it and if you&rsquo;re not then file name hell may be for you! Importantly, version control lets you do as the name suggests and manage different versions of something in a sane easy to implement manner. What is great about it is if you use a plain text based document system, such as LaTeX then you can also manage your documents with it. This makes storing different versions simple (e.g. define a branch as 0.1.0) and you&rsquo;ll always be able to return to that branch for prior iterations.</p>

<h2>Smarter development of objects and functions</h2>

<p>Whenever you need to hard code something in to your scripts you should ask yourself. Do I really need to be doing this? Is there a better way? Can I write this in such a way that I will understand it a week from now, a month from now? Code should be written so that it is easy for you to understand, not the machine. If at a later point you need something to be faster then by all means start optimising things (e.g. move towards C). However, initially write it so that you can understand it. Only rewrite the truly performance critical parts.</p>

<p>Here things which help include, keyword arguments instead of hard coded parameters. Comments, docstrings, shorter functions. Resist the urge to develop so called &ldquo;God&rdquo; functions or classes that do everything. Instead, separate these (where sensible) into smaller reusable functions or methods. Obviously this is context specific and I&rsquo;m not going to give too many specifics.</p>

<p>Just imagine future you is standing behind you with a shotgun and will pull the trigger if he doesn&rsquo;t understand what, or why, you are doing something in particular which is undocumented. The other key element here is to avoid, where possible, scope creep.</p>

<p>Don&rsquo;t just randomly and haphazardly add new methods, or functions to a script and make it do something completely unexpected because it was easier at the time. Separate out the components and then join them together in an intelligent fashion.</p>

<h2>Modules and packages</h2>

<p>Modules and packages are simply reusable code. They&rsquo;re broadly use case independent and are therefore fantastic for saving time. There are hundreds of packages out there which will simplify your life and make you more productive. My advice is to use them.</p>

<p>My second piece of advice is to take those functions which you use again and again and create your own custom packages. You don&rsquo;t have to release this to the general public but a toolbox which makes your life far far easier is not something to be dismissed. The code you write which goes into your module should be the best code you&rsquo;ve written. You&rsquo;re going to be using it hundreds of times and you want to know exactly what it will do in all situations.</p>

<h2>Tests</h2>

<p>Finally we come to the worst aspect of coding from my personal point of view. I know some people love writing tests, and enjoy test driven methodology however I see a key difference here. A lot of the people who enjoy test driven coding are writing software. In research we&rsquo;re often developing a number of small scripts which we use to generate results. Quite simply we don&rsquo;t have the same sense of scale, or requirements, as other programmers.</p>

<p>Therefore, where are tests useful? Tests are most useful in your modules and packages, and less useful in your custom scripts. My general rule of thumb is as follows, if I&rsquo;m going to use this in multiple places and multiple situations then I am going to want a test for each of those places and situations. If I don&rsquo;t do this then I may make a modification for one particular use case forgetting that you&rsquo;re depending upon the previous functionality for a separate usecase. A test, if implemented and properly, would catch this behaviour and you can hopefully develop smarter.</p>

<p>However, the key element here is that you&rsquo;re reusing code. If you are using it once, in a very specific situation then the code itself almost becomes a test. If it works, then the function is correct. If it doesn&rsquo;t work, the function is wrong. As there is only one use case is there much value in developing a test which repeats our use case?</p>

<h2>Conclusions</h2>

<p>In this post I&rsquo;ve argued, and laid out a few examples, of why and how you should be writing more repeatable code as a researcher. The list is by no means complete and is still a work in progress. I am after all still learning how to write better code, something which will continue as long as I am writing code.</p>

<p>Feel free to ask any questions in the comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial Series: Merging Series with Pandas]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/18/tutorial-series-merging-series-with-pandas/"/>
    <updated>2013-05-18T19:03:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/18/tutorial-series-merging-series-with-pandas</id>
    <content type="html"><![CDATA[<p>This post will cover some quick usage surrounding merging a Series and a DataFrame or merging two Series together. This is functionality which I&rsquo;ve often felt is lacking and is boiler plate I&rsquo;ve had to write a number of times. I&rsquo;ve added the functionality to my new repo <a href="github.com/NigelCleland/pdtools">pdtools</a> and should hopefully simplify life for other people.</p>

<p>This post will be a simple explanation as to how it works as well as a brief explanation. Broadly, as some of my work I had to work with extremely large datasets, tens of millions of rows, dozens of columns the full works. I had to merge aspects of these datasets together, often from disparate data sources. one element which was particularly frustrating for me was having large numbers of duplicate columns, and irrelevant information when all I wanted was a single series.</p>

<!-- more -->


<p>So, how do we get around this issue. First a brief overview of merging DataFrames together with Pandas. Of course the pandas documentation covers this in far more depth but in general the code we are interested in is as follows (for index merging)</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Create DataFrames</span>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">,</span><span class="s">&quot;B&quot;</span><span class="p">,</span><span class="s">&quot;C&quot;</span><span class="p">,</span><span class="s">&quot;D&quot;</span><span class="p">])</span>
</span><span class='line'><span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="s">&quot;B&quot;</span><span class="p">]]</span>
</span><span class='line'><span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">&quot;C&quot;</span><span class="p">,</span> <span class="s">&quot;D&quot;</span><span class="p">]]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Merging Method One</span>
</span><span class='line'>
</span><span class='line'><span class="n">df3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">how</span><span class="o">=</span><span class="s">&#39;inner&#39;</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="n">df3</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Should return True</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Method Two</span>
</span><span class='line'>
</span><span class='line'><span class="n">df4</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="n">df4</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Should return True</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Method Three</span>
</span><span class='line'><span class="n">df5</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="n">df5</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Will return False as columns are different</span>
</span><span class='line'><span class="n">df5</span> <span class="o">=</span> <span class="n">df5</span><span class="p">[[</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="s">&quot;B&quot;</span><span class="p">,</span> <span class="s">&quot;C&quot;</span><span class="p">,</span> <span class="s">&quot;D&quot;</span><span class="p">]]</span> <span class="c"># Reorder the columns</span>
</span><span class='line'><span class="n">df5</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Should return True</span>
</span></code></pre></td></tr></table></div></figure>


<p>Broadly, we can either merge two DataFrames using the general pandas merge method, or the specific DataFrame method. Now what if we want to merge a series into a DataFrame, for ecxample.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">s1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&quot;C&quot;</span><span class="p">]</span>
</span><span class='line'><span class="n">s2</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">&quot;D&quot;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Pseudo code</span>
</span><span class='line'><span class="n">df6</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span> <span class="c"># Fails miserably</span>
</span></code></pre></td></tr></table></div></figure>


<p>Broadly, pandas just does not like doing this. However, we can get around this by creating a single column DataFrame from the Series and merging that as follows.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># As a single statement, note this is not the best way to do this.</span>
</span><span class='line'><span class="c"># I&#39;m also taking advantage of the fact that the series has a name which</span>
</span><span class='line'><span class="c"># Will be passed as the new column name.</span>
</span><span class='line'><span class="n">df6</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">s1</span><span class="o">.</span><span class="n">name</span> <span class="p">:</span> <span class="n">s1</span><span class="p">}),</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">s2</span><span class="o">.</span><span class="n">name</span> <span class="p">:</span> <span class="n">s2</span><span class="p">}),</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>Wow, that is ugly. Wouldn&rsquo;t it be simpler if we could just merge the two together. Now, what we can do is create a bit of a wrapper and syntactic sugar around our ugly code above. Let&rsquo;s wrap it in a function to begin with.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">merge_dataframe_series</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
</span><span class='line'>  <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">s</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">s</span><span class="p">}),</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="n">df7a</span> <span class="o">=</span> <span class="n">merge_dataframe_series</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="n">df7</span> <span class="o">=</span> <span class="n">merge_dataframe_series</span><span class="p">(</span><span class="n">df7a</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">df7</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Returns True</span>
</span></code></pre></td></tr></table></div></figure>


<p>Hmmm, slightly, better but still pretty sub par over all. Lets cover the things which are wrong with the first attempt.</p>

<ol>
<li>Pretty unwieldy syntax, have to call it with a DataFrame and Series.</li>
<li>Still need a lot of key word arguments to specify how the merge will occur.</li>
<li>The name is very long, want it much shorter and sweeter.</li>
<li>Ideally we want to be able to call this directly from a DataFrame.</li>
</ol>


<p>So, let&rsquo;s try again.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">def</span> <span class="nf">merge_series</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">series</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
</span><span class='line'>  <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">series</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">series</span><span class="p">}),</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">merge_series</span> <span class="o">=</span> <span class="n">merge_series</span>
</span><span class='line'>
</span><span class='line'><span class="n">df8</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">merge_series</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">merge_series</span><span class="p">(</span><span class="n">s2</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">df8</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Should be True</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now that is much better. The usage is much simpler, we don&rsquo;t need to specify what DataFrame since it is now a method of the DataFrame class. Since we&rsquo;re always going to be using the index of the Series we can automatically incorporate the <code>right_index=True</code> directly into the function. We&rsquo;re still using the **kwds to pass what argument we want for the merge function. Thus we don&rsquo;t need to use the index on the DataFrame, we could use a column using <code>left_on="column"</code> as well.</p>

<p>Let&rsquo;s also do the same for a series</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Use series_merge instead of merge_series to avoid name space error.</span>
</span><span class='line'><span class="c"># We&#39;ll map it to merge_series to maintain the consistent api usage though.</span>
</span><span class='line'><span class="k">def</span> <span class="nf">series_merge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">):</span>
</span><span class='line'>  <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="bp">self</span><span class="p">})</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">other</span><span class="o">.</span><span class="n">name</span> <span class="p">:</span> <span class="n">other</span><span class="p">}),</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="o">.</span><span class="n">merge_series</span> <span class="o">=</span> <span class="n">series_merge</span>
</span><span class='line'>
</span><span class='line'><span class="n">df9</span> <span class="o">=</span> <span class="n">s1</span><span class="o">.</span><span class="n">merge_series</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span>
</span><span class='line'><span class="n">df10</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">df9</span><span class="p">,</span> <span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'><span class="n">df10</span> <span class="o">==</span> <span class="n">df</span> <span class="c"># Should be True</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, since we can only merge series based upon their indices. I&rsquo;m happy to be corrected on this but it is difficult to think of a different use case we can automatically assign the <code>left_index=True</code> and <code>right_index=True</code> key word arguments to simplify it.</p>

<p>Now, what we&rsquo;ve accomplished in this blog post is a couple quick helper functions to make merging Series together, and into DataFrames much much simpler. These are automatically incorporated into the pdtools package. To apply this functionality you can download the package and incorporate it.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pdtools.merging</span> <span class="c"># Just import the two merge functions</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Or</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pdtools</span> <span class="c"># Will import masks as well.</span>
</span></code></pre></td></tr></table></div></figure>


<p>If using the general purpose pdtools import the masks functionality will also be added to the pandas classes. A brief overview of this functionality can be found <a href="http://nigelcleland.github.io/blog/2013/05/12/selecting-data-with-pandas/">here</a>.</p>

<p>Thoughts/Questions/etc, open an issue on <a href="https://github.com/nigelcleland/pdtools/issues">github</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial Series: Using pandas groupby to simplify your life]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/16/tutorial-series-using-pandas-groupby-to-simplify-your-life/"/>
    <updated>2013-05-16T10:40:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/16/tutorial-series-using-pandas-groupby-to-simplify-your-life</id>
    <content type="html"><![CDATA[<p>This is the second instalment in a brief series on using the pandas library for data analysis. The general aim of this series to to highlight some of the cool functionality which is in pandas. None of this is revolutionary, however Wes McKinney and the other contributors have made exceptional contributions to the speed and ease of which it is completed.</p>

<p>In this post I&rsquo;m going to over the groupby function. Highlight a couple of the interesting things which you can do with it and generally show how much simpler pandas makes it to accomplish data analysis. I&rsquo;m going to be using a real data set, hydrology data from the last 80 odd years for the New Zealand electricity grid, this dataset has approximately 30,000 data points (1 per day) and is therefore a good size for this work. This post will cover a brief example of using groupby for time series and won&rsquo;t go through any of the more complex functionality and usage which can be developed when working with full DataFrames.</p>

<!-- more -->


<p>Right to begin we need to load our data, I&rsquo;m going to use a custom module which makes this a bit easier called <a href="http://github.com/NigelCleland/nzem">nzem</a>. The nzem library is my own personal library which simplifies some of the initial data analysis I use most frequently. For example, the following will load hydrology data and automatically parse the relatively disgusting date format which the csv file has. Note, that the nzem module automatically loads and initlises the <a href="http://github.com/NigelCleland/masks">masks</a> library which I discussed in my last post.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Load modules</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">nzem</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Load the data, niwa_date is a specific format not recognised by the dateutil.parser utility</span>
</span><span class='line'><span class="n">hydro_data</span> <span class="o">=</span> <span class="n">nzem</span><span class="o">.</span><span class="n">load_csvfile</span><span class="p">(</span><span class="s">&#39;hydro_data/Hydro_Lake_Data.csv&#39;</span><span class="p">,</span> <span class="n">niwa_date</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Get a single column, daily_level will then be a series</span>
</span><span class='line'><span class="n">daily_level</span> <span class="o">=</span> <span class="n">hydro_data</span><span class="p">[</span><span class="s">&quot;Daily Stored&quot;</span><span class="p">]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Confirmation that we are working with a time series</span>
</span><span class='line'><span class="nb">type</span><span class="p">(</span><span class="n">daily_level</span><span class="p">)</span> <span class="c"># pandas.core.series.Series</span>
</span><span class='line'><span class="nb">type</span><span class="p">(</span><span class="n">daily_level</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="c"># pandas.tseries.index.DatetimeIndex</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, we have a single series which contains the daily storage level of the entire countries hydro lakes (in GWh).</p>

<p>Now, on to using the group by functionality. The pandas <a href="http://pandas.pydata.org/pandas-docs/stable/groupby.html">groupby</a> library has two primary uses. Aggregations and Transformations. An Aggregation will return a new series with indices set by the groupby keys.
A Transformation will return the original series transformed according to the predefined function.</p>

<p>This is best demonstrated by example</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Aggregation</span>
</span><span class='line'><span class="c"># Group by the day of year, take the mean of these points</span>
</span><span class='line'><span class="n">doy</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">dayofyear</span>
</span><span class='line'><span class="n">doy_mean</span> <span class="o">=</span> <span class="n">daily_level</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">doy</span><span class="p">)</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">)</span> <span class="c"># Note you don&#39;t need to use the () at the end of np.mean.</span>
</span><span class='line'>
</span><span class='line'><span class="nb">len</span><span class="p">(</span><span class="n">doy_mean</span><span class="p">)</span> <span class="c"># 366, one for each day</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Transformation</span>
</span><span class='line'><span class="c"># Group by the day of year, but subtract the mean from each point</span>
</span><span class='line'>
</span><span class='line'><span class="n">relative_mean</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class='line'><span class="n">doy_rel_mean</span> <span class="o">=</span> <span class="n">daily_level</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">doy</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">relative_mean</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="nb">len</span><span class="p">(</span><span class="n">doy_rel_mean</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">daily_level</span><span class="p">)</span> <span class="c"># returns True</span>
</span></code></pre></td></tr></table></div></figure>


<p>Now, we need to cover where we use each one. The aggregations are useful when what we are interested in is the summary statistic, and we do not intend to remerge it with our dataframe. For example, if we wanted to remerge it we would have to go through a bunch of steps which would be quite painful. The transform function on the other hand should be used when you want a data point equivalent to each of the original series/dataframe.</p>

<p>For example, in the above example the transform example can be used to create a repeating series which we then apply against our raw data. A visual explanation as to why this is beneficial may be most appropriate.
I&rsquo;m not going to attempt to make this figure extremely pretty, that is for a later post which will cover beautifying matplotlib somewhat. I am chaining methods here which if you are not used to pandas may be slightly confusing. The explanation is that</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Helper functions to reduce line length</span>
</span><span class='line'><span class="n">per10</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span><span class='line'><span class="n">rel_per10</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">-</span> <span class="n">per10</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Create our subplots, share the y axis to make comparison more intuitive</span>
</span><span class='line'><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Plot from 2012 only to avoid too much visual clutter.</span>
</span><span class='line'><span class="c"># Raw Data</span>
</span><span class='line'><span class="n">daily_level</span><span class="o">.</span><span class="n">ix</span><span class="p">[</span><span class="s">&quot;2012&quot;</span><span class="p">:]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Transformation to be applied</span>
</span><span class='line'><span class="n">daily_level</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">doy</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">per10</span><span class="p">)</span><span class="o">.</span><span class="n">ix</span><span class="p">[</span><span class="s">&quot;2012&quot;</span><span class="p">:]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Final Result</span>
</span><span class='line'><span class="n">daily_level</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">doy</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">rel_per10</span><span class="p">)</span><span class="o">.</span><span class="n">ix</span><span class="p">[</span><span class="s">&quot;2012&quot;</span><span class="p">:]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Labels</span>
</span><span class='line'><span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;Raw hydrology data&quot;</span><span class="p">,</span> <span class="s">&quot;Bottom Decile&quot;</span><span class="p">,</span> <span class="s">&quot;Transformed Data&quot;</span><span class="p">]</span>
</span><span class='line'><span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">titles</span><span class="p">):</span>
</span><span class='line'>  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Quantity of Hydro Storage [GWh]&quot;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">&quot;hydro_data_over_time.png&quot;</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">150</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p><img src="http://NigelCleland.github.io/images/may2013/hydro_data_over_time.png"></p>

<p>Right, an explanation. What we have done is create three plots which gives the reader a fairly decent overview as to the hydrology situation from 2012 onwards. From this figure we get an appreciation of the raw situation as seen on a daily basis. In the second plot we can then visually compare this to the bottom decile of what has occurred for the past 80 years. Finally, in the final plot we compare the two sets of data which highlights the low hydro storage experienced during the Autumn and Winter of 2012. It also adequately shows the large series of inflows which occurred over the 2012-2013 new year periods. Finally, the tail end of the figure shows that hydrology levels have again deteriorated from the large burst of inflows.</p>

<p>Not too bad for a few simple lines of code. In total, to produce the image required 14 individual lines excluding the module import.</p>

<p>Hopefully this post has given you a bit more of a detailed explanation as to using the groupby functionality, especially with time series. I&rsquo;ll attempt to extend upon this in the coming days as time and motivation permits.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial Series: Selecting Data with Pandas]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/12/selecting-data-with-pandas/"/>
    <updated>2013-05-12T13:12:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/12/selecting-data-with-pandas</id>
    <content type="html"><![CDATA[<p>As part of my job I need to do a large amount of sample analysis, and visualisation of small subsets of data. This requires a large quantity of iteration, trial and error and repetitive coding. Something we all wish we could avoid, yet sometimes can&rsquo;t. To give a practical example, I often need to work with half hourly data, for a wide range of individual metrics which may have similarities. What I need to do, is isolate this data in various ways.</p>

<p>EDIT: The original post used a library called masks. I&rsquo;ve decided to merge this into a larger directory to be called <a href="http://github.com/NigelCleland/pdtools">pdtools</a>. This directory will contain more helper functions and methods, not just masks.</p>

<!-- more -->


<p>Now, here we introduce pandas. Pandas is an analysis library for Python built on top of numpy for fast merges, joins and analysis. It is an essential part of my day to day work and if you have to work with any decent amount of data I highly recommend using it. However, there is one slight issue I&rsquo;ve had with it. Selecting small subsets of data in a simple fashion. To give an example, in python, to iterate we can use:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">container</span><span class="p">:</span>
</span><span class='line'>    <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
</span><span class='line'>          <span class="k">print</span> <span class="n">item</span>
</span></code></pre></td></tr></table></div></figure>


<p>Here the key is that we&rsquo;re looking at the entire container, and then assessing whether our item fits that subset. However, in pandas it isn&rsquo;t so easy.
In Pandas, to select a subset of the data we create an array of booleans and then apply this to the dataframe as follows:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="s">&quot;B&quot;</span><span class="p">,</span> <span class="s">&quot;C&quot;</span><span class="p">,</span> <span class="s">&quot;D&quot;</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;B&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.2</span><span class="p">)]</span>
</span></code></pre></td></tr></table></div></figure>


<p>In this example, we are selecting a subset of our data which satisfies the condition of column A being less than or equal to 0.5 and column B greater than or equal to 0.2. (Note, the brackets around each statement are required.</p>

<p>However, this syntax, while clear for simple examples does start to break down at higher orders. For example, let&rsquo;s say we had qualitative values in our data frame and we want to select a small subset of them. We cannot use the in operator, or a multiple equal operator. Instead, we need to use the or operator.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Note this won&#39;t work:</span>
</span><span class='line'><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">)]</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Require the following:</span>
</span><span class='line'><span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.3</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.7</span><span class="p">)]</span>
</span></code></pre></td></tr></table></div></figure>


<p>As you can see the amount of duplicate code requires continues to increase linearly with each additional item we wish to check. Is there a better way?
Turns out, maybe.</p>

<p>What I have done is develop a range of common masks and place them in a single <a href="https://github.com/NigelCleland/masks">repository</a> or it may be downloaded using pip</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">pip</span> <span class="n">install</span> <span class="n">masks</span>
</span></code></pre></td></tr></table></div></figure>


<p>To use it, you simply import the masks module after importing pandas and it will apply a number of additional methods to the Series and DataFrame classes in the pandas module. I&rsquo;ve attempted to avoid all known api clashes through the addition of _masks at the end of each function, which should also make the meaning clearer. Using this module our multiple selection example becomes much simpler</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">masks</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class='line'>
</span><span class='line'><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="s">&quot;B&quot;</span><span class="p">,</span> <span class="s">&quot;C&quot;</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Use the in_eqmask this mask will return all values which meet the conditions</span>
</span><span class='line'><span class="c"># the eqmask instead of just mask is to specify that equality conditions are</span>
</span><span class='line'><span class="c"># being introduced</span>
</span><span class='line'>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">in_eqmask</span><span class="p">(</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">24</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Or another simple use case which has frustrated me endlessly in pandas is</span>
</span><span class='line'><span class="c"># returning all rows which satisfy a between type condition.</span>
</span><span class='line'>
</span><span class='line'><span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">&quot;A&quot;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">0.7</span><span class="p">)]</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">bet_mask</span><span class="p">(</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>Which is much much simpler no?</p>

<p>An additional benefit of this is that we are able to chain methods together in order to create the desired subsets. For example.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">ge_mask</span><span class="p">(</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">le_mask</span><span class="p">(</span><span class="s">&quot;C&quot;</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">ne_mask</span><span class="p">(</span><span class="s">&quot;B&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>We just applied three different refinements to get all rows of the dataframe where A is greater than 5, C less than 15 and B not equal to 10.</p>

<p>There are a few more complex functions in the module, such as selecting the top, bottom or middle x% of a particular column or columns e.g.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Take the rows which satisfy the top 25% of column A, then take the bottom 10% of column B</span>
</span><span class='line'><span class="c"># from this subset.</span>
</span><span class='line'><span class="n">df</span><span class="o">.</span><span class="n">top_mask</span><span class="p">(</span><span class="s">&quot;A&quot;</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span><span class="o">.</span><span class="n">bot_mask</span><span class="p">(</span><span class="s">&quot;B&quot;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>masks is still a work in progress, however it is usable as is for prototyping and data exploration. There are a number of additional features which are still needed at this stage. There repo is <a href="https://github.com/NigelCleland/masks">here</a> and all contributions/comments are welcome. I&rsquo;m not sure this is the best way to add this functionality, however it has been useful for me in the past and thus I&rsquo;m putting it out there incase anyone else finds it useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction and blog content]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/09/introduction-and-blog-content/"/>
    <updated>2013-05-09T12:43:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/09/introduction-and-blog-content</id>
    <content type="html"><![CDATA[<p>So, first a brief introduction.
My name is Nigel and I am currently a Ph.D student at the University of Auckland, New Zealand.
My current research is on market based systems for procuring security (reserve) in electricity markets.
In particular, I do a substantial amount of data analysis attempting to assess the consequences and frequency of constrained situations.
I&rsquo;m currently ~1.5 years into this.</p>

<!-- more -->


<p>This blog is intended to serve as predominately writing practice, as well as thoughts on technical and esoteric matters.
I try to use open source software as much as possible in my day to day work with Python, LaTeX and Ubuntu being the main elements.
I would like to contribute more to open source in the future, however I&rsquo;m not entirely sure on the best way to accomplish this.
Hopefully, some blog posts on semi technical matters may provide a stepping stone for this.
I typically use Python for data analysis, especially the incredibly useful pandas and matplotlib libraries.</p>

<p>Outside of this, I love reading, tinkering with and building assorted trinkets/toys and weight lifting.
I plan on using this blog to post on a range of subjects which I have found both interesting and useful over the years.
As such, content may vary greatly.
In particular, I&rsquo;m trying to read a large number of new books with the broadly ambitious aim to read at least one on average every week.
This ambitious tends to often be thwarted by running out of new books to read.
I typically read a lot of print books, not because I am a luddite as I have previously bought, and read, a lot of kindle books.
But mainly because of the concept of ownership and what happens to such books if Amazon or some other company were to change its terms of service.</p>

<p>I am a bit of an information junkie, which I&rsquo;m trying to control as it does tend to consume a lot of my time.
Short form information, which tends to skirt a subject and not really cover it in depth is not exactly a great source of information.
As such, I&rsquo;m attempting to read more literature, long form articles and in depth coverage of topics.
In addition, contributing to these by writing about them is something I want to initiate and this is a good first step.</p>

<p>Finally, I have no set schedule for updating this blog although I will attempt to do so as frequently as my interest permits.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Octopress with Ubuntu]]></title>
    <link href="http://NigelCleland.github.io/blog/2013/05/08/setting-up-octopress/"/>
    <updated>2013-05-08T19:41:00+12:00</updated>
    <id>http://NigelCleland.github.io/blog/2013/05/08/setting-up-octopress</id>
    <content type="html"><![CDATA[<p>Right, so this post will serve as a general overview as to a couple of the difficulties I had when setting up Octopress and Ruby.
I mainly use Python for my day to day analysis work and as such have little experience with Ruby.
I used rbenv for my installation and a couple issues arose which were a bit of a pain to sort out as googling appeared to let me down here.</p>

<!-- more -->


<p>So a couple things. If you are using Ubuntu then you need to use .profile instead of .bash_profile.
If done right the bottom two lines of the following should be.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>cat ~/.profile
</span><span class='line'>
</span><span class='line'>export PATH="$HOME/.rbenv/bin:$PATH
</span><span class='line'>eval "$(rbenv init -)"</span></code></pre></td></tr></table></div></figure>


<p></p>

<p>Another issue which may arise is the difference between the system, and rbenv ruby installation.
The commands you are looking for here are as follows.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>rbenv versions # Will list the installed versions of rbenv
</span><span class='line'>rbenv local &lt;build&gt; # Will set a local version of ruby for the specific folder
</span><span class='line'>rbenv global &lt;build&gt; # Specify the global build of ruby to use.
</span><span class='line'>ruby --version # List the current version of ruby</span></code></pre></td></tr></table></div></figure>


<p>I recommend playing around with each of these commands until you&rsquo;re comfortable switching between different versions of ruby.
In my initial installations I was having a lot of difficulty with different versions of ruby in different locations.
Generally, just a major annoyance to say the least.</p>

<p>Once these two issues were sorted out installation was pretty simple overall.
Hopefully if anyone else is having similar difficulties this post may point them in the right direction.</p>

<p>I&rsquo;ll update this post if needed or if I remember something new.</p>
]]></content>
  </entry>
  
</feed>
